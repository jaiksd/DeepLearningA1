{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1 (2 Marks)\n",
        "\n",
        "Download the fashion-MNIST dataset and plot 1 sample image for each class as shown in the grid below. Use from keras.datasets import fashion_mnist for getting the fashion mnist dataset"
      ],
      "metadata": {
        "id": "4WTBIkeWBjGS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fhsr9dRKnADd"
      },
      "outputs": [],
      "source": [
        "#Importing all the libraries that will be used\n",
        "from keras.datasets import fashion_mnist\n",
        "from keras.datasets import mnist\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "# import warnings\n",
        "# warnings.filterwarnings(\"ignore\")\n",
        "!pip install wandb -qq\n",
        "import wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4IViEkC5X1Ek"
      },
      "outputs": [],
      "source": [
        "wandb.login(key='fbf80504ccef17f5f3b05723be7ea4caff805164')\n",
        "wandb.init(project=\"CS23M030\", name=\"Question_1\")\n",
        "#Load the fashion MNIST data\n",
        "\n",
        "# dataset='mnist'\n",
        "# if dataset=='fashion-mnist':\n",
        "#   (X_train_com, Y_train_com), (X_test, Y_test) = fashion_mnist.load_data()\n",
        "# else:\n",
        "#   (X_train_com, Y_train_com), (X_test, Y_test) = mnist.load_data()\n",
        "\n",
        "\n",
        "(X_train_com, Y_train_com), (X_test, Y_test) = fashion_mnist.load_data()\n",
        "#Normalizing the data\n",
        "X_train_com = X_train_com/255.0\n",
        "X_test = X_test/255.0\n",
        "np.random.seed(137)\n",
        "encoder = OneHotEncoder()\n",
        "\n",
        "#Splitting to get 10% data as validation set\n",
        "X_train, X_val, Y_train, Y_val = train_test_split(X_train_com, Y_train_com, test_size=0.1, random_state=137)\n",
        "\n",
        "Y_train_unencoded = Y_train\n",
        "#One hot encoding of the class labels\n",
        "\n",
        "\n",
        "Y_val = encoder.fit_transform(np.expand_dims(Y_val,1)).toarray()\n",
        "Y_train = encoder.fit_transform(np.expand_dims(Y_train,1)).toarray()\n",
        "Y_test = encoder.fit_transform(np.expand_dims(Y_test,1)).toarray()\n",
        "\n",
        "\n",
        "# Defining the class labels of the 10 classess\n",
        "class_labels={\n",
        "    0: \"T-shirt/top\",\n",
        "    1: \"Trouser\",\n",
        "    2: \"Pullover\",\n",
        "    3: \"Dress\",\n",
        "    4: \"Coat\",\n",
        "    5: \"Sandal\",\n",
        "    6: \"Shirt\",\n",
        "    7: \"Sneaker\",\n",
        "    8: \"Bag\",\n",
        "    9: \"Ankle boot\"\n",
        "}\n",
        "\n",
        "#Let's start plotting one sample plot corressponding to each class\n",
        "plt.figure(figsize=(5,5))\n",
        "for i in range(10):\n",
        "  plt.subplot(5,2,i+1)\n",
        "  #get the first image with the i label\n",
        "  img_index=next(index for index,label in enumerate(Y_train_unencoded) if label==i)\n",
        "  plt.imshow(X_train[img_index],cmap='gray')\n",
        "  plt.title(class_labels[i])\n",
        "  plt.axis('off')\n",
        "  wandb.log({\"examples\": [wandb.Image(X_train[img_index])]})\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2 (10 Marks)\n",
        "\n",
        "Implement a feedforward neural network which takes images from the fashion-mnist data as input and outputs a probability distribution over the 10 classes.\n",
        "\n",
        "Your code should be flexible such that it is easy to change the number of hidden layers and the number of neurons in each hidden layer."
      ],
      "metadata": {
        "id": "QQt6cO-GBnUf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y4_-U4twYB8D"
      },
      "outputs": [],
      "source": [
        "class FeedForwardNN():\n",
        "  \"\"\"\n",
        "  This class implements the forward propagation functionality of a feedforward neural network. It supports flexible hidden layer sizes and four different activation functions.\n",
        "\n",
        "  Attributes:\n",
        "      hidden_layer_sizes: A list containing the sizes of the hidden layers (the length of the list indicates the number of hidden layers in the network).\n",
        "  \"\"\"\n",
        "  def __init__(self, hidden_layer_sizes):\n",
        "    self.weights = {}\n",
        "    self.biases = {}\n",
        "    self.hidden_layer_sizes = hidden_layer_sizes\n",
        "    self.input_layer_size=0\n",
        "    self.output_layer_size=1\n",
        "    # The array of layer sizes will be initialized after obtaining the input and output layer sizes\n",
        "    self.layer_sizes = []\n",
        "\n",
        "    self.A = {}\n",
        "    self.H = {}\n",
        "\n",
        "\n",
        "\n",
        "  def initialize_weights(self):\n",
        "    \"\"\"\n",
        "    Initializes the weights between the layers of the network. Weights are randomly initialized.\n",
        "    \"\"\"\n",
        "    self.layer_sizes = [self.input_layer_size] + self.hidden_layer_sizes + [self.output_layer_size]\n",
        "    len_hl=len(self.hidden_layer_sizes)\n",
        "    weight_counts = len_hl +1\n",
        "    np.random.seed(137)\n",
        "    np.random.RandomState(137)\n",
        "    i=0\n",
        "    while i<(weight_counts):\n",
        "      # Initialize weights for each layer randomly as a matrix of size (previous layer size) * (next layer size)\n",
        "      self.weights[i+1] = np.random.randn(self.layer_sizes[i], self.layer_sizes[i+1])\n",
        "     # Initialize biases for each layer to 0 as a matrix of size 1 * (next layer size)\n",
        "      self.biases[i+1] = np.zeros((1, self.layer_sizes[i+1]))\n",
        "      i+=1\n",
        "\n",
        "  def calculate_activation(self, name, X):\n",
        "    \"\"\"\n",
        "    Calculates the activation based on the specified activation function name. This function in turn calls the required activation function.\n",
        "    \"\"\"\n",
        "    if name==\"ReLU\":\n",
        "      s=self.ReLU(X)\n",
        "      return s\n",
        "\n",
        "    if name==\"sigmoid\":\n",
        "      s=self.sigmoid(X)\n",
        "      return s\n",
        "\n",
        "    if name==\"tanh\":\n",
        "      s=self.tanh(X)\n",
        "      return s\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  #calculate the ReLU function\n",
        "  def ReLU(self,X):\n",
        "    return X * (X > 0)\n",
        "  # calculating the softmax function\n",
        "\n",
        "  #function used\n",
        "  def u1():\n",
        "    return 1\n",
        "  def softmax(self, X):\n",
        "    X_max = np.max(X)\n",
        "    exponentials = np.exp(X - X_max)\n",
        "    return exponentials / np.sum(exponentials)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  # Utility function to calculate the sigmoid function\n",
        "  def sigmoid(self,X):\n",
        "    return 1.0/(1.0+np.exp(-X))\n",
        "\n",
        "  def identity(self, X):\n",
        "  # Utility to calculate identity function\n",
        "    return X\n",
        "\n",
        "  #calculating the cross entropy funtion\n",
        "  def cross_entropy(self,Y_true,Y_pred):\n",
        "    loss=np.multiply(Y_pred,Y_true)\n",
        "    loss=loss[loss!=0]\n",
        "    loss=-np.log(loss)\n",
        "    loss=np.mean(loss)\n",
        "    return loss\n",
        "\n",
        "\n",
        "\n",
        "  #calculating the tanh funtion\n",
        "  def tanh(self,X):\n",
        "    return np.tanh(X)\n",
        "\n",
        "\n",
        "\n",
        "  \"\"\"\n",
        "   Performing forward propagation in the data X.\n",
        "  \"\"\"\n",
        "  def forward_prop(self, activation, X):\n",
        "\n",
        "\n",
        "    self.H = {}\n",
        "    #set i=0 for the while loop\n",
        "    i=0\n",
        "    self.A = {}\n",
        "    #Make a row vector intialize the output from input layer as H[0] into a single row\n",
        "    self.H[0] = X.reshape(1,-1)\n",
        "\n",
        "    #formula for calculation\n",
        "    #formula applied:a(x) = W_x*h(x-1) + b and h(a(x)) = activation(a(x)) for hidden layer\n",
        "\n",
        "    while i<(len(self.hidden_layer_sizes)):\n",
        "      mat = np.matmul(self.H[i], self.weights[i+1])\n",
        "      self.A[i+1] =  mat + self.biases[i+1]\n",
        "      cal=self.calculate_activation(activation, self.A[i+1])\n",
        "      self.H[i+1] = cal\n",
        "      i+=1\n",
        "\n",
        "    # calculating hadamard and softmax at the output layer\n",
        "    # for the output layer a(x) and h(a(x)) = softmax(a(x))\n",
        "    mul=np.matmul(self.H[len(self.hidden_layer_sizes)], self.weights[len(self.hidden_layer_sizes)+1])\n",
        "    self.A[len(self.hidden_layer_sizes)+1] = mul + self.biases[len(self.hidden_layer_sizes)+1]\n",
        "\n",
        "    self.H[len(self.hidden_layer_sizes)+1] = self.softmax(self.A[len(self.hidden_layer_sizes)+1])\n",
        "    return\n",
        "\n",
        "\n",
        "  def fit(self, activation, X, Y):\n",
        "    \"\"\"\n",
        "    This method trains the model with the given data (X, Y) by performing a single forward pass and producing the probabilities calculated at the output layer.\n",
        "    \"\"\"\n",
        "\n",
        "    # columns in output (label count)\n",
        "    self.output_layer_size = Y.shape[1]\n",
        "\n",
        "    # features in data(features)\n",
        "    self.input_layer_size = X.shape[1]*X.shape[1]\n",
        "\n",
        "    output_prob = []\n",
        "    self.initialize_weights()\n",
        "\n",
        "    for x, y in zip(X, Y):\n",
        "      #Forward Propogation\n",
        "      self.forward_prop(activation, x)\n",
        "\n",
        "      output_prob.append(self.H[len(self.hidden_layer_sizes)+1][0])\n",
        "    return output_prob\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4d4LzCyHYOpR"
      },
      "outputs": [],
      "source": [
        "#Sample implementation using 3 hidden layers of sizes 100, 256 and 512 respectively\n",
        "\n",
        "# model = FeedForwardNN([128,128,128])\n",
        "# activation_name = \"ReLU\"\n",
        "# class_predictions = model.fit(activation_name, X_train, Y_train)\n",
        "# #Class probabilities for 0th indexed image\n",
        "\n",
        "# np.set_printoptions(suppress=True)\n",
        "# print(class_predictions[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usJzPKjwgNhp"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vtiaxg1jYlOO"
      },
      "source": [
        "Question 3 (24 Marks)\n",
        "\n",
        "Implement the backpropagation algorithm with support for the following optimisation functions\n",
        "\n",
        "    sgd\n",
        "    momentum based gradient descent\n",
        "    nesterov accelerated gradient descent\n",
        "    rmsprop\n",
        "    adam\n",
        "    nadam\n",
        "\n",
        "(12 marks for the backpropagation framework and 2 marks for each of the optimisation algorithms above)\n",
        "\n",
        "We will check the code for implementation and ease of use (e.g., how easy it is to add a new optimisation algorithm such as Eve). Note that the code should be flexible enough to work with different batch sizes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uV2Xq9MhYnQK"
      },
      "outputs": [],
      "source": [
        "# #An independent function which calculated accuracy given the true and predicted class labels\n",
        "def accuracy(y_true, y_pred):\n",
        "  \"\"\"\n",
        "  calculating the accuracy\n",
        "  \"\"\"\n",
        "  acc = np.sum(np.equal(y_true,y_pred))/y_true.shape[0]\n",
        "  return acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4AImChLhYq34"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Class for implementing the feedforward neural network's forward propagation functionality.\n",
        "\"\"\"\n",
        "class FeedForwardNN():\n",
        "  def __init__(self, hidden_layer_sizes, optimizer, activation_function, output_activation, loss_function, epochs = 1, batch_size = 4, initialization = \"Random\", log=0, console_log = 1, train_losses_list = None, train_accuracy_list = None, val_losses_list = None, val_accuracy_list = None):\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    - log: Log onto WandB (default 0).\n",
        "    - console_log: Log onto console (default 1).\n",
        "    -self: Reference to the object itself.\n",
        "    - hidden_layer_sizes: List indicating the sizes of hidden layers (number of hidden layers for the network).\n",
        "    \"\"\"\n",
        "    self.hidden_layer_sizes = hidden_layer_sizes\n",
        "    self.optimizer = optimizer\n",
        "    self.input_layer_size=0\n",
        "    self.output_layer_size=1\n",
        "    self.epochs = epochs\n",
        "    #- batch_size: Batch size (default 1024).\n",
        "    self.batch_size = batch_size\n",
        "    self.loss_function = loss_function\n",
        "    self.dw = {}\n",
        "    self.db = {}\n",
        "    self.activation_function = activation_function\n",
        "    self.output_activation = output_activation\n",
        "\n",
        "\n",
        "    #- initialization: Weight initialization method (default \"Random\").\n",
        "    self.initialization = initialization\n",
        "    #Layer sizes array will be initialzed after input and output layer size is obtained\n",
        "    self.weights = {}\n",
        "    self.biases = {}\n",
        "    self.val_losses_list = val_losses_list\n",
        "    self.val_accuracy_list = val_accuracy_list\n",
        "    self.log = log\n",
        "    self.console_log = console_log\n",
        "    self.layer_sizes = []\n",
        "\n",
        "    self.dA = {}\n",
        "    self.dH = {}\n",
        "    self.A = {}\n",
        "    self.H = {}\n",
        "\n",
        "\n",
        "    self.train_losses_list = train_losses_list\n",
        "    self.train_accuracy_list = train_accuracy_list\n",
        "\n",
        "\n",
        "\n",
        "  def initialize_weights(self):\n",
        "    \"\"\"\n",
        "    initilizing the weights\n",
        "    \"\"\"\n",
        "    np.random.seed(137)\n",
        "    np.random.RandomState(137)\n",
        "    self.layer_sizes = [self.input_layer_size] + self.hidden_layer_sizes + [self.output_layer_size]\n",
        "    weight_counts = len(self.hidden_layer_sizes)\n",
        "    weight_counts+=1\n",
        "\n",
        "    self.optimizer.initialize(self.layer_sizes)\n",
        "    #defining a utility function\n",
        "    def utility_x():\n",
        "      return 1;\n",
        "    i=0\n",
        "    while i<(weight_counts):\n",
        "\n",
        "      if self.initialization == \"Xavier\":\n",
        "        # using the xavier method to iniliatize the weights\n",
        "        limit = np.sqrt(2 / float(self.layer_sizes[i] + self.layer_sizes[i+1]))\n",
        "        self.weights[i+1] = np.random.normal(0.0, limit, size=(self.layer_sizes[i], self.layer_sizes[i+1]))\n",
        "\n",
        "\n",
        "      if self.initialization == \"random\":\n",
        "\n",
        "        self.weights[i+1] = np.random.randn(self.layer_sizes[i], self.layer_sizes[i+1])\n",
        "\n",
        "\n",
        "      #setting the biases\n",
        "      self.biases[i+1] = np.zeros((1, self.layer_sizes[i+1]))\n",
        "      i+=1\n",
        "\n",
        "\n",
        "\n",
        "  def forward_prop(self, X):\n",
        "    \"\"\"\n",
        "    performing forward propagation on the data X\n",
        "    \"\"\"\n",
        "    self.H = {}\n",
        "    self.A = {}\n",
        "\n",
        "    i=0\n",
        "    #initializing i for while loop\n",
        "    self.H[0] = X.reshape(1,-1)\n",
        "\n",
        "\n",
        "    while i<(len(self.hidden_layer_sizes)):\n",
        "      mat=np.matmul(self.H[i], self.weights[i+1])\n",
        "      fin=mat+self.biases[i+1]\n",
        "      self.A[i+1] =  fin\n",
        "      self.H[i+1] = self.activation_function.calculate_activation(self.A[i+1])\n",
        "      i+=1\n",
        "\n",
        "    #perform a(x) and h(a(x)) = softmax(a(x))\n",
        "    temp=np.matmul(self.H[len(self.hidden_layer_sizes)], self.weights[len(self.hidden_layer_sizes)+1])\n",
        "    ad=temp + self.biases[len(self.hidden_layer_sizes)+1]\n",
        "    self.A[len(self.hidden_layer_sizes)+1] =  ad\n",
        "    self.H[len(self.hidden_layer_sizes)+1] = self.output_activation.calculate_activation(self.A[len(self.hidden_layer_sizes)+1])\n",
        "    return\n",
        "\n",
        "  def back_prop(self, X, Y,  dw_i, db_i):\n",
        "    \"\"\"\n",
        "    performing backward propogation .\n",
        "    \"\"\"\n",
        "    #calculate the gradient of loss wrt the activation of output layer\n",
        "    self.dA[len(self.hidden_layer_sizes)+1] = self.loss_function.last_output_derivative(self.H[len(self.hidden_layer_sizes)+1], Y, self.output_activation.calculate_derivative(self.A[len(self.hidden_layer_sizes)+1]))\n",
        "\n",
        "    #using the formula taught in the class,that is applying chain rule here\n",
        "    for i in range(len(self.hidden_layer_sizes), -1, -1):\n",
        "      dw_i[i+1] = np.matmul(self.H[i].T, self.dA[i+1])\n",
        "\n",
        "      db_i[i+1] = self.dA[i+1]\n",
        "      if i!=0:\n",
        "\n",
        "        self.dH[i] = np.matmul(self.dA[i+1],self.weights[i+1].T)\n",
        "\n",
        "        self.dA[i] = np.multiply(self.activation_function.calculate_derivative(self.A[i]), self.dH[i])\n",
        "\n",
        "    return dw_i, db_i\n",
        "\n",
        "\n",
        "  def fit(self, X, Y, X_val, Y_val):\n",
        "    \"\"\"\n",
        "    func to fit the data (X,Y) on the model. This performs forward + backward pass for epoch number of times. Gradient is updated after each batch is processed.\n",
        "    \"\"\"\n",
        "    # columns in output (label count)\n",
        "    self.output_layer_size = Y.shape[1]\n",
        "    self.input_layer_size = X.shape[1]*X.shape[1] # Number of features in data(features)\n",
        "\n",
        "    self.initialize_weights()\n",
        "    for e in range(self.epochs):\n",
        "      y_preds = []\n",
        "\n",
        "      count = -1\n",
        "      for i in range(len(self.hidden_layer_sizes)+1):\n",
        "        self.dw[i+1] = np.zeros((self.layer_sizes[i], self.layer_sizes[i+1]))\n",
        "        self.db[i+1] = np.zeros((1, self.layer_sizes[i+1]))\n",
        "\n",
        "      for x, y in zip(X, Y):\n",
        "        dw_i = {}\n",
        "        db_i = {}\n",
        "        count += 1\n",
        "\n",
        "        if count==self.batch_size:\n",
        "          #Done wih current batch\n",
        "          count = 0\n",
        "          #checking it the optimizer is nag\n",
        "          if self.optimizer.optimizer_name()==\"nag\":\n",
        "            w_look_ahead = {}\n",
        "            b_look_ahead = {}\n",
        "            for i in range(len(self.hidden_layer_sizes)+1):\n",
        "              sub_w= self.optimizer.beta*self.optimizer.update_history_w[i+1]\n",
        "              w_look_ahead[i+1] = self.weights[i+1] - sub_w\n",
        "              sub_b=self.optimizer.beta*self.optimizer.update_history_b[i+1]\n",
        "              b_look_ahead[i+1] = self.biases[i+1] -sub_b\n",
        "\n",
        "            #updating the biases\n",
        "            biases_old = self.biases\n",
        "            #updating the weights\n",
        "            weights_old = self.weights\n",
        "\n",
        "            self.weights = w_look_ahead\n",
        "            self.biases = b_look_ahead\n",
        "            self.forward_prop(x)\n",
        "            dw_look_ahead, db_look_ahead = self.back_prop(x,y, dw_i, db_i)\n",
        "            self.weights, self.biases = self.optimizer.update_parameters(weights_old, biases_old, dw_look_ahead, db_look_ahead, self.hidden_layer_sizes)\n",
        "\n",
        "          else:\n",
        "            self.weights, self.biases = self.optimizer.update_parameters(self.weights, self.biases, self.dw, self.db, self.hidden_layer_sizes)\n",
        "          for i in range(len(self.hidden_layer_sizes)+1):\n",
        "            self.dw[i+1] = np.zeros((self.layer_sizes[i], self.layer_sizes[i+1]))\n",
        "            self.db[i+1] = np.zeros((1, self.layer_sizes[i+1]))\n",
        "\n",
        "\n",
        "        #Forward Propogation\n",
        "        self.forward_prop(x)\n",
        "\n",
        "\n",
        "\n",
        "        #Predictions\n",
        "        y_preds.append(self.H[len(self.hidden_layer_sizes)+1])\n",
        "\n",
        "        #Backward Propogation using Loss funtion\n",
        "        self.back_prop(x,y, dw_i, db_i)\n",
        "\n",
        "        #defining a function for calculating the derivative(NT)\n",
        "        def derivative_of_x(x):\n",
        "          exponentials = np.exp(x)\n",
        "          return exponentials / np.sum(exponentials)\n",
        "\n",
        "        for i in range(len(self.hidden_layer_sizes)+1):\n",
        "          self.dw[i+1] += dw_i[i+1]\n",
        "          self.db[i+1] += db_i[i+1]\n",
        "\n",
        "      #Update weights based on loss(GD hence once every epoch update)\n",
        "      if self.optimizer.optimizer_name()==\"nag\":\n",
        "        w_look_ahead = {}\n",
        "        b_look_ahead = {}\n",
        "        for i in range(len(self.hidden_layer_sizes)+1):\n",
        "          w_look_ahead[i+1] = self.weights[i+1] - self.optimizer.beta*self.optimizer.update_history_w[i+1]\n",
        "          b_look_ahead[i+1] = self.biases[i+1] - self.optimizer.beta*self.optimizer.update_history_b[i+1]\n",
        "\n",
        "        weights_old = self.weights\n",
        "        biases_old = self.biases\n",
        "        self.weights = w_look_ahead\n",
        "        self.biases = b_look_ahead\n",
        "        self.forward_prop(x)\n",
        "        dw_look_ahead, db_look_ahead = self.back_prop(x,y, dw_i, db_i)\n",
        "        self.weights, self.biases = self.optimizer.update_parameters(weights_old, biases_old, dw_look_ahead, db_look_ahead, self.hidden_layer_sizes)\n",
        "\n",
        "      else:\n",
        "        self.weights, self.biases = self.optimizer.update_parameters(self.weights, self.biases, self.dw, self.db, self.hidden_layer_sizes)\n",
        "      y_preds = np.array(y_preds).squeeze()\n",
        "      y_preds_validation = self.predict(X_val)\n",
        "      #getting all the losses and updation\n",
        "      training_loss = self.loss_function.calculate_loss(Y, y_preds, self.batch_size)\n",
        "      validation_loss = self.loss_function.calculate_loss(Y_val, y_preds_validation, self.batch_size)\n",
        "      #getting accuracy\n",
        "      training_accuracy = accuracy(np.argmax(Y,1), np.argmax(y_preds,1))\n",
        "      validation_accuracy = accuracy(np.argmax(Y_val,1), np.argmax(y_preds_validation,1))\n",
        "      if self.log==1:\n",
        "        #Log metrics to wandb\n",
        "        wandb.log({\"Training_accuracy\": training_accuracy, \"Validation_accuracy\": validation_accuracy, \"Training_loss\": training_loss, \"Validation_loss\": validation_loss, 'Epoch': e+1})\n",
        "      if self.console_log == 1:\n",
        "        #Log results to console\n",
        "        print(\"Epoch: \",e+1,\" Training Loss: \",training_loss, \" Validation Loss:\",validation_loss ,\" Training Accuracy: \",training_accuracy, \" Validation Accuracy:\", validation_accuracy)\n",
        "      if self.val_accuracy_list != None:\n",
        "        self.val_accuracy_list.append(validation_accuracy)\n",
        "      if self.val_losses_list != None:\n",
        "        self.val_losses_list.append(validation_loss)\n",
        "      if self.train_accuracy_list != None:\n",
        "        self.train_accuracy_list.append(training_accuracy)\n",
        "      if self.train_losses_list != None:\n",
        "        self.train_losses_list.append(training_loss)\n",
        "\n",
        "    return training_loss, validation_loss, training_accuracy, validation_accuracy\n",
        "\n",
        "  def predict(self, X):\n",
        "\n",
        "    out_prob = []\n",
        "    #function to predict and storing the result in the out_prob\n",
        "    for x in X:\n",
        "      values = self.forward_prop(x)\n",
        "      predictions = self.H[len(self.hidden_layer_sizes)+1]\n",
        "      out_prob.append(predictions)\n",
        "\n",
        "    out_prob = np.array(out_prob).squeeze()\n",
        "    return out_prob\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VkyzxGJxYwFL"
      },
      "outputs": [],
      "source": [
        "class StocasticGD():\n",
        "  \"\"\"\n",
        "  This class implements the Stochastic Gradient Descent (SGD) optimizer.\n",
        "\n",
        "  Parameters:\n",
        "\n",
        "    learning_rate: The rate at which the optimizer updates the weights during training.\n",
        "    weight_decay: The factor by which the optimizer reduces the weights to combat overfitting.\n",
        "    \"\"\"\n",
        "  def __init__(self, learning_rate = 0.001, weight_decay = 0.0):\n",
        "    self.learning_rate = learning_rate\n",
        "    self.weight_decay = weight_decay\n",
        "\n",
        "  def set_learning_rate(self, learning_rate):\n",
        "    self.learning_rate = learning_rate\n",
        "  def set_uts():\n",
        "    a=np.ones(5)\n",
        "    return a\n",
        "\n",
        "  def set_initial_parameters(self, parameters):\n",
        "    self.learning_rate = parameters[\"learning_rate\"]\n",
        "    self.weight_decay = parameters[\"weight_decay\"]\n",
        "\n",
        "  def set_weight_decay(self, weight_decay):\n",
        "    self.weight_decay = weight_decay\n",
        "\n",
        "\n",
        "  def optimizer_name(self):\n",
        "    return \"gd\"\n",
        "\n",
        "  def initialize(self, all_layers):\n",
        "    return\n",
        "\n",
        "\n",
        "  #this function updates all the parameters\n",
        "  #like weights,biases,dw,db,layers\n",
        "  def update_parameters(self, weights, biases, dw, db, layers):\n",
        "\n",
        "    i=0\n",
        "    #formula I used below is same as what we taught in class that is upadating weights and biases for every datapoints\n",
        "    while i<(len(layers)+1):\n",
        "        dw[i+1] = dw[i+1] + self.weight_decay*weights[i+1]\n",
        "\n",
        "\n",
        "        weights[i+1] = weights[i+1] - self.learning_rate * dw[i+1]\n",
        "\n",
        "        biases[i+1] = biases[i+1] - self.learning_rate * db[i+1]\n",
        "        i+=1\n",
        "    return weights, biases\n",
        "\n",
        "\n",
        "class MomentumGD():\n",
        "  \"\"\"\n",
        "  This class defines the Momentum Gradient Descent optimizer.\n",
        "\n",
        "  Parameters:\n",
        "\n",
        "    learning_rate: The rate at which the optimizer adjusts the weights during training.\n",
        "    weight_decay: The factor by which the optimizer reduces the weights to prevent overfitting.\n",
        "    beta: The momentum parameter for the optimizer.\n",
        "  \"\"\"\n",
        "  def __init__(self, learning_rate = 0.001, beta = 0.001, weight_decay = 0.0):\n",
        "    self.learning_rate = learning_rate\n",
        "    self.update_history_w = {}\n",
        "    self.update_history_b = {}\n",
        "    self.beta = beta\n",
        "    self.initialized = False\n",
        "    self.weight_decay = weight_decay\n",
        "\n",
        "\n",
        "  def optimizer_name(self):\n",
        "    return \"momentum\"\n",
        "\n",
        "\n",
        "  def set_weight_decay(self, weight_decay):\n",
        "    self.weight_decay = weight_decay\n",
        "\n",
        "  def initialize(self, all_layers):\n",
        "    self.update_history_w.clear()\n",
        "    self.update_history_b.clear()\n",
        "    i=0\n",
        "    while i<(len(all_layers)-1):\n",
        "      self.update_history_w[i+1] = np.zeros((all_layers[i], all_layers[i+1]))\n",
        "      self.update_history_b[i+1] = np.zeros((1, all_layers[i+1]))\n",
        "      i+=1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def set_learning_rate(self, learning_rate):\n",
        "    self.learning_rate = learning_rate\n",
        "\n",
        "  def set_initial_parameters(self, parameters):\n",
        "    self.learning_rate = parameters[\"learning_rate\"]\n",
        "    self.beta = parameters[\"beta\"]\n",
        "    self.weight_decay = parameters[\"weight_decay\"]\n",
        "\n",
        "\n",
        "  def update_parameters(self, weights, biases, dw, db, layers):\n",
        "    \"\"\"\n",
        "    updating the parameters\n",
        "    \"\"\"\n",
        "    i=0\n",
        "    while i<(len(layers)+1):\n",
        "        dw[i+1] = dw[i+1] + self.weight_decay*weights[i+1]\n",
        "\n",
        "        self.update_history_w[i+1] =self.beta*self.update_history_w[i+1] + self.learning_rate*dw[i+1]\n",
        "        weights[i+1] = weights[i+1] - self.update_history_w[i+1]\n",
        "\n",
        "        self.update_history_b[i+1] =self.beta*self.update_history_b[i+1] + self.learning_rate*db[i+1]\n",
        "        biases[i+1] = biases[i+1] - self.update_history_b[i+1]\n",
        "        i+=1\n",
        "    return weights, biases\n",
        "\n",
        "\n",
        "class NAG():\n",
        "  \"\"\"\n",
        "  This class is for implementing the Nesterov accelerated Gradient Descent optimizer.\n",
        "\n",
        "  Parameters:\n",
        "\n",
        "    learning_rate: The rate at which the optimizer adjusts the weights during training.\n",
        "    beta: The momentum parameter for the optimizer.\n",
        "    \"\"\"\n",
        "  def __init__(self, learning_rate = 0.001, beta = 0.9):\n",
        "    self.w_look_ahead={}\n",
        "    self.beta = beta\n",
        "    #setting the look_ahead symbol\n",
        "    self.b_look_ahead={}\n",
        "\n",
        "    self.db_look_ahead={}\n",
        "    #setting the learning rate\n",
        "    self.learning_rate = learning_rate\n",
        "\n",
        "    self.initialized = False\n",
        "    self.update_history_w = {}\n",
        "    self.dw_look_ahead={}\n",
        "    self.update_history_b = {}\n",
        "\n",
        "  def optimizer_name(self):\n",
        "    return \"nag\"\n",
        "\n",
        "\n",
        "\n",
        "  def initialize(self, all_layers):\n",
        "    self.w_look_ahead.clear()\n",
        "    self.update_history_b.clear()\n",
        "    #clearing all the parameters\n",
        "    self.b_look_ahead.clear()\n",
        "    self.update_history_w.clear()\n",
        "    self.db_look_ahead.clear()\n",
        "    self.dw_look_ahead.clear()\n",
        "\n",
        "\n",
        "    for i in range(len(all_layers)-1):\n",
        "      self.update_history_w[i+1] = np.zeros((all_layers[i], all_layers[i+1]))\n",
        "      self.update_history_b[i+1] = np.zeros((1, all_layers[i+1]))\n",
        "      self.dw_look_ahead[i+1] = np.zeros((all_layers[i], all_layers[i+1]))\n",
        "      self.db_look_ahead[i+1] = np.zeros((1, all_layers[i+1]))\n",
        "      self.w_look_ahead[i+1] = np.zeros((all_layers[i], all_layers[i+1]))\n",
        "      self.b_look_ahead[i+1] = np.zeros((1, all_layers[i+1]))\n",
        "\n",
        "\n",
        "\n",
        "  def set_initial_parameters(self, parameters):\n",
        "    self.learning_rate = parameters[\"learning_rate\"]\n",
        "    self.beta = parameters[\"beta\"]\n",
        "\n",
        "  def set_learning_rate(self, learning_rate):\n",
        "    self.learning_rate = learning_rate\n",
        "\n",
        "\n",
        "\n",
        "  def update_parameters(self, weights, biases, dw, db, layers):\n",
        "    \"\"\"\n",
        "    updating the parameters\n",
        "    \"\"\"\n",
        "    i=0\n",
        "    while i<(len(layers)+1):\n",
        "        self.update_history_w[i+1] = self.beta*self.update_history_w[i+1] + self.learning_rate*dw[i+1]\n",
        "        self.update_history_b[i+1] = self.beta*self.update_history_b[i+1] + self.learning_rate*db[i+1]\n",
        "        weights[i+1] = weights[i+1] - self.update_history_w[i+1]\n",
        "        biases[i+1] = biases[i+1] - self.update_history_b[i+1]\n",
        "        i+=1\n",
        "\n",
        "    return weights, biases\n",
        "\n",
        "\n",
        "class RMSProp():\n",
        "  \"\"\"\n",
        "  This class implements the RMSProp optimizer.\n",
        "\n",
        "  Parameters:\n",
        "\n",
        "    learning_rate: The rate at which the optimizer adjusts the weights during training.\n",
        "    weight_decay: Weight decay parameter for regularization.\n",
        "    beta: The momentum parameter for the optimizer.\n",
        "    epsilon: A small value added to the denominator to prevent division by zero.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "  def __init__(self, learning_rate = 0.001, beta = 0.001, epsilon = 1e-8, weight_decay = 0.0):\n",
        "    self.learning_rate = learning_rate\n",
        "    #_init_ function initializes all the parameters\n",
        "    self.v_w = {}\n",
        "    self.v_b = {}\n",
        "    self.epsilon = epsilon\n",
        "    self.beta = beta\n",
        "\n",
        "\n",
        "    self.initialized = False\n",
        "    self.weight_decay = weight_decay\n",
        "\n",
        "  def initialize(self, all_layers):\n",
        "    self.v_b.clear()\n",
        "    self.v_w.clear()\n",
        "\n",
        "    i=0\n",
        "    while i<(len(all_layers)-1):\n",
        "      self.v_w[i+1] = np.zeros((all_layers[i], all_layers[i+1]))\n",
        "      self.v_b[i+1] = np.zeros((1, all_layers[i+1]))\n",
        "      i+=1\n",
        "\n",
        "  def set_weight_decay(self, weight_decay):\n",
        "    self.weight_decay = weight_decay\n",
        "\n",
        "  def optimizer_name(self):\n",
        "    return \"rmsprop\"\n",
        "\n",
        "  def set_learning_rate(self, learning_rate):\n",
        "    self.learning_rate = learning_rate\n",
        "\n",
        "\n",
        "\n",
        "  def set_initial_parameters(self, parameters):\n",
        "    self.learning_rate = parameters[\"learning_rate\"]\n",
        "    self.beta = parameters[\"beta\"]\n",
        "    self.epsilon = parameters[\"epsilon\"]\n",
        "    self.weight_decay = parameters[\"weight_decay\"]\n",
        "\n",
        "\n",
        "  def update_parameters(self, weights, biases, dw, db, layers):\n",
        "    \"\"\"\n",
        "    updating the parameters\n",
        "    \"\"\"\n",
        "    i=0\n",
        "    while i<(len(layers)+1):\n",
        "      #iteratively updating the weights and biases\n",
        "        dw[i+1] = dw[i+1] + self.weight_decay*weights[i+1]\n",
        "        self.v_w[i+1] =self.beta*self.v_w[i+1] + (1-self.beta)* ((dw[i+1])**2)\n",
        "        self.v_b[i+1] =self.beta*self.v_b[i+1] + (1-self.beta)* ((db[i+1])**2)\n",
        "\n",
        "        weights[i+1] = weights[i+1] - ((self.learning_rate)/np.sqrt(self.v_w[i+1] + self.epsilon))*dw[i+1]\n",
        "        biases[i+1] = biases[i+1] - ((self.learning_rate)/np.sqrt(self.v_b[i+1] + self.epsilon))*db[i+1]\n",
        "        i+=1\n",
        "    return weights, biases\n",
        "\n",
        "\n",
        "class Adam():\n",
        "  \"\"\"\n",
        "  This class defines the Adam optimizer.\n",
        "\n",
        "  Parameters:\n",
        "\n",
        "    learning_rate: The rate at which the optimizer adjusts the weights during training.\n",
        "    weight_decay: Weight decay parameter for regularization.\n",
        "    beta1: The exponential decay rate for the first moment estimates.\n",
        "    beta2: The exponential decay rate for the second moment estimates.\n",
        "    epsilon: A small value added to the denominator to prevent division by zero.\n",
        "    \"\"\"\n",
        "  def __init__(self, learning_rate = 0.001,beta1 = 0.9, beta2 = 0.999, epsilon = 1e-8, weight_decay = 0.0):\n",
        "    self.learning_rate = learning_rate\n",
        "    self.beta1 = beta1\n",
        "    self.beta2 = beta2\n",
        "    self.v_w = {}\n",
        "    self.v_b = {}\n",
        "    self.epsilon = epsilon\n",
        "    self.initialized = False\n",
        "    self.iterations = 1\n",
        "    self.weight_decay = weight_decay\n",
        "\n",
        "    self.m_w = {}\n",
        "    self.m_b = {}\n",
        "\n",
        "\n",
        "  def initialize(self, all_layers):\n",
        "    self.v_w.clear()\n",
        "    self.v_b.clear()\n",
        "    self.m_w.clear()\n",
        "    self.m_b.clear()\n",
        "    i=0\n",
        "    while i<(len(all_layers)-1):\n",
        "      self.v_w[i+1] = np.zeros((all_layers[i], all_layers[i+1]))\n",
        "      self.v_b[i+1] = np.zeros((1, all_layers[i+1]))\n",
        "      self.m_w[i+1] = np.zeros((all_layers[i], all_layers[i+1]))\n",
        "      self.m_b[i+1] = np.zeros((1, all_layers[i+1]))\n",
        "      i+=1\n",
        "\n",
        "  def set_learning_rate(self, learning_rate):\n",
        "    self.learning_rate = learning_rate\n",
        "\n",
        "  def optimizer_name(self):\n",
        "    return \"adam\"\n",
        "\n",
        "  def set_weight_decay(self, weight_decay):\n",
        "    self.weight_decay = weight_decay\n",
        "\n",
        "\n",
        "\n",
        "  def set_initial_parameters(self, parameters):\n",
        "    self.learning_rate = parameters[\"learning_rate\"]\n",
        "    self.beta1 = parameters[\"beta1\"]\n",
        "    self.beta2 = parameters[\"beta2\"]\n",
        "    self.epsilon = parameters[\"epsilon\"]\n",
        "    self.weight_decay = parameters[\"weight_decay\"]\n",
        "\n",
        "\n",
        "  def update_parameters(self, weights, biases, dw, db, layers):\n",
        "    \"\"\"\n",
        "    updating the parameters\n",
        "    \"\"\"\n",
        "    i=0\n",
        "    while i<(len(layers)+1):\n",
        "\n",
        "        dw[i+1] = dw[i+1] + self.weight_decay*weights[i+1]\n",
        "\n",
        "        self.m_w[i+1] = self.beta1*self.m_w[i+1] + (1-self.beta1)* (dw[i+1])\n",
        "        self.m_b[i+1] = self.beta1*self.m_b[i+1] + (1-self.beta1)* (db[i+1])\n",
        "        self.v_w[i+1] = self.beta2*self.v_w[i+1] + (1-self.beta2)* ((dw[i+1])**2)\n",
        "        self.v_b[i+1] = self.beta2*self.v_b[i+1] + (1-self.beta2)* ((db[i+1])**2)\n",
        "\n",
        "        #this is the formula taught in the class\n",
        "        m_w_hat = self.m_w[i+1]/(1-(self.beta1**self.iterations))\n",
        "        m_b_hat = self.m_b[i+1]/(1-(self.beta1**self.iterations))\n",
        "        v_w_hat = self.v_w[i+1]/(1-(self.beta2**self.iterations))\n",
        "        v_b_hat = self.v_b[i+1]/(1-(self.beta2**self.iterations))\n",
        "\n",
        "        #updating the weights\n",
        "        weights[i+1] = weights[i+1] - ((self.learning_rate)/(np.sqrt(v_w_hat) + self.epsilon))*(m_w_hat)\n",
        "        biases[i+1] = biases[i+1] - ((self.learning_rate)/(np.sqrt(v_b_hat) + self.epsilon))*(m_b_hat)\n",
        "        i+=1\n",
        "    #moving to next iteration\n",
        "    self.iterations += 1\n",
        "\n",
        "    return weights, biases\n",
        "\n",
        "\n",
        "class Nadam():\n",
        "\n",
        "  \"\"\"\n",
        "  Defines the NAdam optimizer.\n",
        "\n",
        "  Parameters:\n",
        "\n",
        "    learning_rate: Rate at which the optimizer updates the weights during training.\n",
        "    weight_decay: Weight decay parameter for regularization.\n",
        "    beta1: Exponential decay rate for the first moment estimates.\n",
        "    beta2: Exponential decay rate for the second moment estimates.\n",
        "    epsilon: Small value added to the denominator to prevent division by zero.\n",
        "    \"\"\"\n",
        "  def __init__(self, learning_rate = 0.001, beta1 = 0.9, beta2 = 0.999, epsilon = 1e-8, weight_decay = 0.0):\n",
        "    self.learning_rate = learning_rate\n",
        "    self.beta1 = beta1\n",
        "    self.beta2 = beta2\n",
        "    self.v_w = {}\n",
        "    self.v_b = {}\n",
        "    self.epsilon = epsilon\n",
        "    self.initialized = False\n",
        "\n",
        "    self.m_w = {}\n",
        "    self.m_b = {}\n",
        "    self.iterations = 1\n",
        "    self.weight_decay = weight_decay\n",
        "\n",
        "  def initialize(self, all_layers):\n",
        "    self.v_w.clear()\n",
        "    self.v_b.clear()\n",
        "    self.m_w.clear()\n",
        "    self.m_b.clear()\n",
        "    i=0\n",
        "    while i<(len(all_layers)-1):\n",
        "      self.v_w[i+1] = np.zeros((all_layers[i], all_layers[i+1]))\n",
        "      self.v_b[i+1] = np.zeros((1, all_layers[i+1]))\n",
        "      self.m_w[i+1] = np.zeros((all_layers[i], all_layers[i+1]))\n",
        "      self.m_b[i+1] = np.zeros((1, all_layers[i+1]))\n",
        "      i+=1\n",
        "\n",
        "  def set_learning_rate(self, learning_rate):\n",
        "    self.learning_rate = learning_rate\n",
        "\n",
        "  def set_weight_decay(self, weight_decay):\n",
        "    self.weight_decay = weight_decay\n",
        "\n",
        "  def optimizer_name(self):\n",
        "    return \"nadam\"\n",
        "\n",
        "  def set_initial_parameters(self, parameters):\n",
        "    self.learning_rate = parameters[\"learning_rate\"]\n",
        "    self.beta1 = parameters[\"beta1\"]\n",
        "    self.beta2 = parameters[\"beta2\"]\n",
        "    self.epsilon = parameters[\"epsilon\"]\n",
        "    self.weight_decay = parameters[\"weight_decay\"]\n",
        "\n",
        "\n",
        "  def update_parameters(self, weights, biases, dw, db, layers):\n",
        "    \"\"\"\n",
        "    updating the parameters\n",
        "    \"\"\"\n",
        "    i=0\n",
        "    while i<(len(layers)+1):\n",
        "\n",
        "        #this is the gradient\n",
        "        dw[i+1] = dw[i+1] + self.weight_decay*weights[i+1]\n",
        "\n",
        "        self.m_w[i+1] = self.beta1*self.m_w[i+1] + (1-self.beta1)* (dw[i+1])\n",
        "        self.m_b[i+1] = self.beta1*self.m_b[i+1] + (1-self.beta1)* (db[i+1])\n",
        "\n",
        "        self.v_w[i+1] = self.beta2*self.v_w[i+1] + (1-self.beta2)* ((dw[i+1])**2)\n",
        "        self.v_b[i+1] = self.beta2*self.v_b[i+1] + (1-self.beta2)* ((db[i+1])**2)\n",
        "\n",
        "        #calculating the m_w_hat and m_b_hat\n",
        "        m_w_hat = self.m_w[i+1]/(1-(self.beta1**self.iterations))\n",
        "        m_b_hat = self.m_b[i+1]/(1-(self.beta1**self.iterations))\n",
        "\n",
        "         #calculating the v_w_hat and v_b_hat\n",
        "        v_w_hat = self.v_w[i+1]/(1-(self.beta2**self.iterations))\n",
        "        v_b_hat = self.v_b[i+1]/(1-(self.beta2**self.iterations))\n",
        "\n",
        "        #updating the weights and biases\n",
        "        weights[i+1] = weights[i+1] - ((self.learning_rate)/(np.sqrt(v_w_hat) + self.epsilon))*(self.beta1 * m_w_hat + ((1-self.beta1)/(1-(self.beta1**self.iterations))*dw[i+1]))\n",
        "        biases[i+1] = biases[i+1] - ((self.learning_rate)/(np.sqrt(v_b_hat) + self.epsilon))*(self.beta1 * m_b_hat + ((1-self.beta1)/(1-(self.beta1**self.iterations))*db[i+1]))\n",
        "        i+=1\n",
        "\n",
        "    self.iterations += 1\n",
        "\n",
        "    return weights, biases\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rad--tkxYy6E"
      },
      "outputs": [],
      "source": [
        "class Softmax():\n",
        "  \"\"\"\n",
        "  Class of Softmax activation function.\n",
        "  \"\"\"\n",
        "  def calculate_derivative(self, X):\n",
        "    softmax = self.calculate_activation(X)\n",
        "    return softmax*(1-softmax)\n",
        "\n",
        "  def calculate_activation(self, X):\n",
        "    #Utility to calculate softmax function\n",
        "    X_max = np.max(X)\n",
        "    exponentials = np.exp(X - X_max)\n",
        "    return exponentials / np.sum(exponentials)\n",
        "\n",
        "\n",
        "class Sigmoid():\n",
        "  \"\"\"\n",
        "  Implementing Sigmoid activation function.\n",
        "  \"\"\"\n",
        "  def calculate_derivative(self, X):\n",
        "    val = self.calculate_activation(X)\n",
        "    return val*(1-val)\n",
        "\n",
        "  def calculate_activation(self, X):\n",
        "    return 1.0/(1.0+np.exp(-X))\n",
        "\n",
        "\n",
        "class Tanh():\n",
        "  \"\"\"\n",
        "  implementing Tanh activation function.\n",
        "  \"\"\"\n",
        "  def calculate_derivative(self,X):\n",
        "    s=1 - (np.tanh(X) ** 2)\n",
        "    return s\n",
        "\n",
        "  def calculate_activation(self,X):\n",
        "    s=np.tanh(X)\n",
        "    return s\n",
        "\n",
        "\n",
        "class Identity():\n",
        "    def calculate_activation(self, X):\n",
        "        return X\n",
        "\n",
        "    def calculate_derivative(self, X):\n",
        "        # Derivative of the identity function is always 1\n",
        "        return np.ones_like(X)\n",
        "\n",
        "class ReLU():\n",
        "  \"\"\"\n",
        "  Class to implement ReLU activation function.\n",
        "  \"\"\"\n",
        "  def calculate_derivative(self,X):\n",
        "    X[X <= 0.0] = 0.0\n",
        "    X[X > 0.0] = 1.0\n",
        "    return X\n",
        "  def calculate_activation(self,X):\n",
        "    return X * (X > 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_CA3hmKzY1UE"
      },
      "outputs": [],
      "source": [
        "class CrossEntropy():\n",
        "  \"\"\"\n",
        "  cross entropy loss function\n",
        "\n",
        "  \"\"\"\n",
        "  #this function returns the name cross_entropy_loss\n",
        "  def name(self):\n",
        "    return \"cross_entropy_loss\"\n",
        "\n",
        "  def calculate_loss(self, Y_true, Y_pred, batch_size):\n",
        "    #this function calculates the loss\n",
        "    for p in Y_pred[0]:\n",
        "      if np.isnan(p) or p<10e-8:\n",
        "        p=10e-8\n",
        "    loss=np.multiply(Y_pred,Y_true)\n",
        "    loss=loss[loss!=0]\n",
        "    loss=-np.log(loss)\n",
        "    loss=np.mean(loss)\n",
        "\n",
        "    return loss\n",
        "\n",
        "  def calculate_derivative(self, Y_pred,Y_true):\n",
        "    return -Y_true/(Y_pred)\n",
        "\n",
        "  def last_output_derivative(self, Y_pred,Y_true,activation_derivative):\n",
        "    for p in Y_pred[0]:\n",
        "      if np.isnan(p) or p<10e-8:\n",
        "        p=10e-8\n",
        "    return -(Y_true - Y_pred)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class SquaredErrorLoss():\n",
        "  \"\"\"\n",
        "  squared loss function\n",
        "  \"\"\"\n",
        "\n",
        "  def name(self):\n",
        "    #this function returns the name of the loss\n",
        "    return \"squared_loss\"\n",
        "\n",
        "  def calculate_derivative(self, Y_pred,Y_true):\n",
        "    return (Y_pred)*(Y_pred-Y_true)/len(Y_true)\n",
        "  def calculate_loss(self, Y_true, Y_pred, batch_size):\n",
        "    return (1/2) * np.sum((Y_pred-Y_true)**2)/len(Y_true)\n",
        "\n",
        "\n",
        "\n",
        "  def last_output_derivative(self, Y_pred,Y_true, activation_derivative):\n",
        "    for p in Y_pred[0]:\n",
        "      if np.isnan(p) or p<10e-8:\n",
        "        p=10e-8\n",
        "    return (Y_pred - Y_true)*activation_derivative/len(Y_true)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kWsD5RXEY4uH"
      },
      "outputs": [],
      "source": [
        "#Add layer sizes for the hidden layers\n",
        "layers = [32, 32, 32]\n",
        "\n",
        "optimizers = {\"gradient_descent\":StocasticGD(), \"momentum_gd\":MomentumGD(), \"nag\":NAG(), \"rmsprop\":RMSProp(), \"adam\":Adam(), \"nadam\":Nadam()}\n",
        "loss_functions = {\"cross_entropy\":CrossEntropy(), \"squared_loss\":SquaredErrorLoss()}\n",
        "activation_functions = {\"sigmoid\": Sigmoid(), \"softmax\":Softmax(), \"tanh\": Tanh(), \"ReLU\":ReLU()}\n",
        "\n",
        "\n",
        "#Select optimizer(momentum)\n",
        "optimizer_momentum = optimizers[\"momentum_gd\"]\n",
        "optimizer_parameters_momentum = {\"learning_rate\":0.0001, \"beta\":0.6, \"weight_decay\":0}\n",
        "#optimizer_parameters_momentum = {\"learning_rate\":0.01, \"beta\":0.9}\n",
        "\n",
        "optimizer_momentum.set_initial_parameters(optimizer_parameters_momentum)\n",
        "\n",
        "#Select optimizer(nag)\n",
        "optimizer_nag = optimizers[\"nag\"]\n",
        "optimizer_parameters_nag = {\"learning_rate\":0.001, \"beta\":0.9}\n",
        "optimizer_nag.set_initial_parameters(optimizer_parameters_nag)\n",
        "\n",
        "#Select optimizer(sgd)\n",
        "optimizer_sgd = optimizers[\"gradient_descent\"]\n",
        "optimizer_parameters_sgd = {\"learning_rate\":0.001, \"weight_decay\":0.5}\n",
        "optimizer_sgd.set_initial_parameters(optimizer_parameters_sgd)\n",
        "\n",
        "#Select optimizer(rmsprop)\n",
        "optimizer_rmsprop = optimizers[\"rmsprop\"]\n",
        "optimizer_parameters_rmsprop = {\"learning_rate\":0.01, \"beta\":0.9, \"epsilon\":1e-8, \"weight_decay\":0.5}\n",
        "optimizer_rmsprop.set_initial_parameters(optimizer_parameters_rmsprop)\n",
        "\n",
        "\n",
        "#Select optimizer(nadam)\n",
        "optimizer_nadam = optimizers[\"nadam\"]\n",
        "optimizer_parameters_nadam = {\"learning_rate\":0.0001, \"beta1\":0.09, \"beta2\":0.999, \"epsilon\":1e-8, \"weight_decay\":0}\n",
        "optimizer_nadam.set_initial_parameters(optimizer_parameters_nadam)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Select optimizer(adam)\n",
        "optimizer_adam = optimizers[\"adam\"]\n",
        "optimizer_parameters_adam = {\"learning_rate\":0.0001, \"beta1\":0.09, \"beta2\":0.999, \"epsilon\":1e-8, \"weight_decay\":0.5}\n",
        "optimizer_adam.set_initial_parameters(optimizer_parameters_adam)\n",
        "\n",
        "\n",
        "#Select loss function\n",
        "loss_cross_entropy = loss_functions[\"cross_entropy\"]\n",
        "loss_squared = loss_functions[\"squared_loss\"]\n",
        "\n",
        "#Select activation(hidden layers)\n",
        "activation_sigmoid = activation_functions[\"sigmoid\"]\n",
        "activation_softmax = activation_functions[\"softmax\"]\n",
        "activation_tanh = activation_functions[\"tanh\"]\n",
        "activation_ReLU = activation_functions[\"ReLU\"]\n",
        "activation_iden=Identity()\n",
        "\n",
        "#Select activation(output layer)\n",
        "output_activation_softmax = activation_functions[\"softmax\"]\n",
        "\n",
        "# model = FeedForwardNN(layers, optimizer_adam, activation_ReLU, output_activation_softmax, loss_squared, 5, 512, initialization = \"Xavier\")\n",
        "# model.fit(X_train, Y_train, X_val, Y_val)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "5C4FbA9FBb1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "e9RooG7fBeI-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aSKjsy_LY7oz"
      },
      "outputs": [],
      "source": [
        "# #Accuracy for test data\n",
        "# y_preds = model.predict(X_test)\n",
        "# print(accuracy(np.argmax(Y_test,1), np.argmax(y_preds,1)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMFxL6VIZA2X"
      },
      "source": [
        "Question 4 (10 Marks)\n",
        "\n",
        "Use the sweep functionality provided by wandb to find the best values for the hyperparameters listed below. Use the standard train/test split of fashion_mnist (use (X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()). Keep 10% of the training data aside as validation data for this hyperparameter search. Here are some suggestions for different values to try for hyperparameters. As you can quickly see that this leads to an exponential number of combinations. You will have to think about strategies to do this hyperparameter search efficiently. Check out the options provided by wandb.sweep and write down what strategy you chose and why.\n",
        "\n",
        "    number of epochs: 5, 10\n",
        "    number of hidden layers: 3, 4, 5\n",
        "    size of every hidden layer: 32, 64, 128\n",
        "    weight decay (L2 regularisation): 0, 0.0005, 0.5\n",
        "    learning rate: 1e-3, 1 e-4\n",
        "    optimizer: sgd, momentum, nesterov, rmsprop, adam, nadam\n",
        "    batch size: 16, 32, 64\n",
        "    weight initialisation: random, Xavier\n",
        "    activation functions: sigmoid, tanh, ReLU\n",
        "\n",
        "wandb will automatically generate the following plots. Paste these plots below using the \"Add Panel to Report\" feature. Make sure you use meaningful names for each sweep (e.g. hl_3_bs_16_ac_tanh to indicate that there were 3 hidden layers, batch size was 16 and activation function was ReLU) instead of using the default names (whole-sweep, kind-sweep) given by wandb."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#,'identity'"
      ],
      "metadata": {
        "id": "KNCAog7RXTrE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QURNtnfSZCLl"
      },
      "outputs": [],
      "source": [
        "sweep_config = {\n",
        "    #random over the hyperparameters\n",
        "    'name':'sweep2',\n",
        "    'method': 'random',\n",
        "    'metric': {\n",
        "      'name': 'accuracy',\n",
        "      'goal': 'maximize'\n",
        "    },\n",
        "    'parameters': {\n",
        "        'dataset':{\n",
        "            'values': ['fashion-mnist']\n",
        "        },\n",
        "\n",
        "        'optimizer': {\n",
        "            'values': ['sgd', 'momentum', 'nag', 'rmsprop', 'adam', 'nadam']\n",
        "        },\n",
        "        'epochs': {\n",
        "            'values': [5,10]\n",
        "        },\n",
        "        'loss': {\n",
        "            'values': ['mean_squared_error', 'cross_entropy']\n",
        "        },\n",
        "        'learning_rate': {\n",
        "            'values': [ 0.001, 0.0001]\n",
        "        },\n",
        "\n",
        "        'weight_decay': {\n",
        "            'values': [0.0, 0.0005, 0.5]\n",
        "        },\n",
        "        'momentum' : {\n",
        "            'values': [0.5]\n",
        "        },\n",
        "        'beta': {\n",
        "          'values' : [0.5]\n",
        "        },\n",
        "        'beta1' : {\n",
        "            'values' : [0.5]\n",
        "        },\n",
        "        'beta2' : {\n",
        "            'values' : [0.5]\n",
        "        },\n",
        "        'epsilon' :{\n",
        "            'values' : [0.000001]\n",
        "        },\n",
        "\n",
        "        'activation': {\n",
        "            'values': ['sigmoid' , 'tanh', 'ReLU']\n",
        "        },\n",
        "        'num_layers':{\n",
        "            'values' : [3,4,5]\n",
        "        },\n",
        "\n",
        "        'hidden_size': {\n",
        "            'values': [32,64,128]\n",
        "        },\n",
        "        'batch_size':{\n",
        "            'values':[16,32,64]\n",
        "        },\n",
        "        'weight_init':{\n",
        "            'values':['random','Xavier']\n",
        "        }\n",
        "    }\n",
        "}\n",
        "def train():\n",
        "\n",
        "      config_defaults = {\n",
        "          'epochs': 1,\n",
        "          'learning_rate': 0.1,\n",
        "          'momentum' : 0.5,\n",
        "          'beta':0.5,\n",
        "          'beta1':0.5,\n",
        "          'beta2':0.5,\n",
        "          'epsilon': 0.000001,\n",
        "          'weight_init':'random',\n",
        "          'num_layers':1,\n",
        "          'hidden_size':4,\n",
        "          'optimizer':'sgd',\n",
        "          'activation':'sigmoid',\n",
        "          'loss' : 'cross_entropy',\n",
        "          'weight_decay' : 0.0,\n",
        "          'batch_size':4\n",
        "      }\n",
        "\n",
        "\n",
        "    # with wandb.init(project='CS23M030',config = sweep_config, name=\"Question_345\"):\n",
        "      parameters = wandb.init(project='CS23M030',config = sweep_config, name=\"Question_4\")\n",
        "      config=parameters.config\n",
        "      output = []\n",
        "      hidden_size = config.hidden_size\n",
        "      num_layers = config.num_layers\n",
        "\n",
        "      for _ in range(num_layers):\n",
        "          output.append(hidden_size)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      optimizers = {\"gradient_descent\":StocasticGD(), \"momentum_gd\":MomentumGD(), \"nag\":NAG(), \"rmsprop\":RMSProp(), \"adam\":Adam(), \"nadam\":Nadam()}\n",
        "      loss_functions = {\"cross_entropy\":CrossEntropy(), \"mean_squared_error\":SquaredErrorLoss()}\n",
        "      activation_functions = {\"sigmoid\": Sigmoid(), \"softmax\":Softmax(), \"tanh\": Tanh(), \"ReLU\":ReLU(),\"identity\":Identity()}\n",
        "\n",
        "      #Setting custom run name\n",
        "      wandb.run.name = 'epochs_' + str(config.epochs) + '_activation_' + config.activation + '_optimizer_' + config.optimizer + '_layers_' + str(len(output)) +'_decay_' + str(config.weight_decay) + '_beta_' + str(config.beta) + '_learning_rate_' + str(config.learning_rate) + '_batch_size_' + str(config.batch_size)\n",
        "\n",
        "      # Config is a variable that holds and saves hyperparameters and inputs\n",
        "      learning_rate = config.learning_rate\n",
        "      weight_decay = config.weight_decay\n",
        "      beta = config.beta\n",
        "      beta1=config.beta1\n",
        "      beta2=config.beta2\n",
        "      momentum=config.momentum\n",
        "      epochs = config.epochs\n",
        "      hidden_size = config.hidden_size\n",
        "      if config.loss==\"cross_entropy\":\n",
        "        loss_cross_entropy=CrossEntropy()\n",
        "      else:\n",
        "        loss_cross_entropy=SquaredErrorLoss()\n",
        "\n",
        "\n",
        "      if config.activation==\"sigmoid\":\n",
        "        activation = activation_functions[\"sigmoid\"]\n",
        "\n",
        "      if config.activation==\"tanh\":\n",
        "        activation = activation_functions[\"tanh\"]\n",
        "\n",
        "      if config.activation==\"ReLU\":\n",
        "        activation = activation_functions[\"ReLU\"]\n",
        "      if config.activation==\"identity\":\n",
        "        activation = activation_functions[\"identity\"]\n",
        "\n",
        "      output_activation = activation_functions[\"softmax\"]\n",
        "\n",
        "      if config.optimizer==\"sgd\":\n",
        "        #optimizer selected is sgd\n",
        "        optimizer = optimizers[\"gradient_descent\"]\n",
        "        optimizer_parameters_sgd = {\"learning_rate\":config.learning_rate, \"weight_decay\":weight_decay}\n",
        "        optimizer.set_initial_parameters(optimizer_parameters_sgd)\n",
        "\n",
        "      if config.optimizer==\"momentum\":\n",
        "        #optimizer selected is momentum\n",
        "        optimizer = optimizers[\"momentum_gd\"]\n",
        "        optimizer_parameters_momentum = {\"learning_rate\":config.learning_rate, \"beta\":config.momentum, \"weight_decay\":weight_decay}\n",
        "        optimizer.set_initial_parameters(optimizer_parameters_momentum)\n",
        "\n",
        "      if config.optimizer==\"nag\":\n",
        "        #optimizer selected is nag\n",
        "        optimizer = optimizers[\"nag\"]\n",
        "        optimizer_parameters_nag = {\"learning_rate\":config.learning_rate, \"beta\":config.momentum}\n",
        "        optimizer.set_initial_parameters(optimizer_parameters_nag)\n",
        "\n",
        "      if config.optimizer==\"rmsprop\":\n",
        "        #optimizer selected is rmsprop\n",
        "        optimizer = optimizers[\"rmsprop\"]\n",
        "        optimizer_parameters_rmsprop = {\"learning_rate\":config.learning_rate, \"beta\":config.beta, \"epsilon\":1e-8, \"weight_decay\":weight_decay}\n",
        "        optimizer.set_initial_parameters(optimizer_parameters_rmsprop)\n",
        "\n",
        "      if config.optimizer==\"adam\":\n",
        "        #optimizer selected is adam\n",
        "        optimizer = optimizers[\"adam\"]\n",
        "        optimizer_parameters_adam = {\"learning_rate\":config.learning_rate, \"beta1\":config.beta1, \"beta2\":config.beta2, \"epsilon\":1e-8, \"weight_decay\":weight_decay}\n",
        "        optimizer.set_initial_parameters(optimizer_parameters_adam)\n",
        "\n",
        "      if config.optimizer==\"nadam\":\n",
        "        #optimizer selected is nadam\n",
        "        optimizer = optimizers[\"nadam\"]\n",
        "        optimizer_parameters_nadam = {\"learning_rate\":config.learning_rate, \"beta1\":config.beta1, \"beta2\":config.beta2, \"epsilon\":1e-8, \"weight_decay\":weight_decay}\n",
        "        optimizer.set_initial_parameters(optimizer_parameters_nadam)\n",
        "\n",
        "\n",
        "      batch_size = config.batch_size\n",
        "      initialization = config.weight_init\n",
        "      #Training the model\n",
        "      #Ensure that log=1 parameter is set for logging onto wandb\n",
        "      model = FeedForwardNN(output, optimizer, activation, output_activation, loss_cross_entropy, epochs, batch_size, initialization , log=1, console_log = 1)\n",
        "\n",
        "      train_loss, val_loss, train_accuracy, val_accuracy = model.fit(X_train, Y_train, X_val, Y_val)\n",
        "      wandb.log({\"accuracy\": val_accuracy})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p9eSPLwZZJJP"
      },
      "outputs": [],
      "source": [
        "sweep_id = wandb.sweep(sweep_config, project=\"CS23M030\")\n",
        "wandb.agent(sweep_id, train, count = 20)\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQ0fwjDDaW5U"
      },
      "source": [
        "Question 7 (10 Marks)\n",
        "\n",
        "For the best model identified above, report the accuracy on the test set of fashion_mnist and plot the confusion matrix as shown below. More marks for creativity (less marks for producing the plot shown below as it is)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "21h_JyOYaYPg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2e7d5a2-66a3-46cb-d238-6b6d5398b0df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:  1  Training Loss:  0.5283252235244414  Validation Loss: 0.46606844126169655  Training Accuracy:  0.8081666666666667  Validation Accuracy: 0.831\n"
          ]
        }
      ],
      "source": [
        "#running the model for the best parameters to get the best model\n",
        "#Add layer sizes for the hidden layers\n",
        "layers = [128, 128, 128]\n",
        "\n",
        "\n",
        "optimizer_parameters_adam = {\"learning_rate\":0.001, \"beta1\":0.09, \"beta2\":0.999, \"epsilon\":1e-8, \"weight_decay\":0.0005}\n",
        "\n",
        "\n",
        "#activation selection\n",
        "activation_ReLU = ReLU()\n",
        "\n",
        "#Loss function selection\n",
        "loss_cross_entropy = CrossEntropy()\n",
        "\n",
        "\n",
        "\n",
        "#optimizer selected is adam\n",
        "optimizer_adam = Adam()\n",
        "optimizer_adam.set_initial_parameters(optimizer_parameters_adam)\n",
        "\n",
        "#for output layer selecting softmax\n",
        "output_activation_softmax = Softmax()\n",
        "\n",
        "model = FeedForwardNN(layers, optimizer_adam, activation_ReLU, output_activation_softmax, loss_cross_entropy, 3, 64, initialization = \"Xavier\")\n",
        "model.fit(X_train, Y_train, X_val, Y_val)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "woP31NjxabT-"
      },
      "outputs": [],
      "source": [
        "#calculating accuracy for the test data\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Accuracy for Test data\", accuracy(np.argmax(Y_test,1), np.argmax(y_pred,1)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zkLAr7JsacOI"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "#plotting the confusion matrix with the best parameters\n",
        "wandb.init(project=\"CS23M030\", name=\"Question_7\")\n",
        "\n",
        "conf_matrix=metrics.confusion_matrix(np.argmax(Y_test,1), np.argmax(y_pred,1))\n",
        "df_conf_matrix = pd.DataFrame(conf_matrix, index=[i for i in class_labels], columns=[i for i in class_labels])\n",
        "\n",
        "a=10\n",
        "plt.figure(figsize=(a, a))\n",
        "ax = sns.heatmap(df_conf_matrix, annot=True,  cmap='YlGnBu', fmt='d',linewidths=3, linecolor='black')\n",
        "ax.set_yticklabels(class_labels,rotation=0)\n",
        "plt.xlabel(\"True Class\")\n",
        "plt.ylabel(\"Predicted Class\")\n",
        "plt.title('Confusion Matrix of best model on FASHION-MNIST Dataset', fontsize=20)\n",
        "\n",
        "wandb.log({\"Confusion_matrix\": wandb.Image(plt)})\n",
        "\n",
        "plt.show()\n",
        "\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOwEm4qzakju"
      },
      "source": [
        "Question 8 (5 Marks)\n",
        "\n",
        "In all the models above you would have used cross entropy loss. Now compare the cross entropy loss with the squared error loss. I would again like to see some automatically generated plots or your own plots to convince me whether one is better than the other."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O9Bandoyal3P"
      },
      "outputs": [],
      "source": [
        "\n",
        "#creating a dictionary for all the functions\n",
        "\n",
        "\n",
        "optimizers = {\"gradient_descent\":StocasticGD(), \"momentum_gd\":MomentumGD(), \"nag\":NAG(), \"rmsprop\":RMSProp(), \"adam\":Adam(), \"nadam\":Nadam()}\n",
        "loss_functions = {\"cross_entropy\":CrossEntropy(), \"squared_loss\":SquaredErrorLoss()}\n",
        "activation_functions = {\"sigmoid\": Sigmoid(), \"softmax\":Softmax(), \"tanh\": Tanh(), \"ReLU\":ReLU()}\n",
        "\n",
        "layers = [128, 128 , 128]\n",
        "#optimizer selected is adam\n",
        "optimizer_adam_1 = Adam()\n",
        "optimizer_parameters_adam = {\"learning_rate\":0.001, \"beta1\":0.09, \"beta2\":0.999, \"epsilon\":1e-8, \"weight_decay\":0.0005}\n",
        "optimizer_adam_1.set_initial_parameters(optimizer_parameters_adam)\n",
        "\n",
        "#optimizer selected is adam\n",
        "optimizer_adam_2 = Adam()\n",
        "optimizer_parameters_adam = {\"learning_rate\":0.001, \"beta1\":0.09, \"beta2\":0.999, \"epsilon\":1e-8, \"weight_decay\":0.0005}\n",
        "optimizer_adam_2.set_initial_parameters(optimizer_parameters_adam)\n",
        "\n",
        "#loss function selection\n",
        "loss_cross_entropy = loss_functions[\"cross_entropy\"]\n",
        "loss_squared = loss_functions[\"squared_loss\"]\n",
        "\n",
        "#activation function selection\n",
        "activation_ReLU = activation_functions[\"ReLU\"]\n",
        "\n",
        "#for the output layer setting the softmax function\n",
        "output_activation_softmax = activation_functions[\"softmax\"]\n",
        "\n",
        "train_losses_crossentropy = []\n",
        "train_accuracy_crossentropy = []\n",
        "val_losses_crossentropy = []\n",
        "val_accuracy_crossentropy = []\n",
        "\n",
        "train_losses_squareloss = []\n",
        "train_accuracy_squareloss = []\n",
        "val_losses_squareloss = []\n",
        "val_accuracy_squareloss = []\n",
        "\n",
        "model_crossEnt = FeedForwardNN(layers, optimizer_adam_1, activation_ReLU, output_activation_softmax, loss_cross_entropy, 3, 64, initialization = \"Xavier\", train_losses_list = train_losses_crossentropy, train_accuracy_list = train_accuracy_crossentropy, val_losses_list = val_losses_crossentropy, val_accuracy_list = val_accuracy_crossentropy)\n",
        "model_squared = FeedForwardNN(layers, optimizer_adam_2, activation_ReLU, output_activation_softmax, loss_squared, 3, 64, initialization = \"Xavier\", train_losses_list = train_losses_squareloss, train_accuracy_list = train_accuracy_squareloss, val_losses_list = val_losses_squareloss, val_accuracy_list = val_accuracy_squareloss)\n",
        "\n",
        "model_crossEnt.fit(X_train, Y_train, X_val, Y_val)\n",
        "print(\"-----------------------------------------------------------------------------------\")\n",
        "model_squared.fit(X_train, Y_train, X_val, Y_val)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M13Wl1fPaufx"
      },
      "outputs": [],
      "source": [
        "wandb.init(project=\"CS23M030\", name=\"Question_8\")\n",
        "#plotting cross entropy and squared error\n",
        "epochs = list(range(1, len(train_losses_squareloss)+1))\n",
        "\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.plot(epochs, train_losses_crossentropy, 'r', label ='Cross Entropy')\n",
        "plt.plot(epochs, train_losses_squareloss, 'b', label ='Squared Loss')\n",
        "plt.legend()\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Train Loss per epoch\")\n",
        "plt.title('Train Loss comparison for Squared Loss and Crossentropy Loss', fontsize=20)\n",
        "wandb.log({\"Train Loss per epoch for Squared v/s CrossEntropy Loss \": wandb.Image(plt)})\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.plot(epochs, train_accuracy_crossentropy, 'r', label ='Cross Entropy')\n",
        "plt.plot(epochs, train_accuracy_squareloss, 'b', label ='Squared Loss')\n",
        "plt.legend()\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Train accuracy per epoch\")\n",
        "plt.title('Train accuracy comparison for Squared Loss and Crossentropy Loss', fontsize=20)\n",
        "wandb.log({\"Train accuracy per epoch for Squared v/s CrossEntropy Loss \": wandb.Image(plt)})\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.plot(epochs, val_losses_crossentropy, 'r', label ='Cross Entropy')\n",
        "plt.plot(epochs, val_losses_squareloss, 'b', label ='Squared Loss')\n",
        "plt.legend()\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Validation Loss per epoch\")\n",
        "plt.title('Validation Loss comparison for Squared Loss and Crossentropy Loss', fontsize=20)\n",
        "wandb.log({\"Validation Loss per epoch for Squared v/s CrossEntropy Loss \": wandb.Image(plt)})\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.plot(epochs, val_accuracy_crossentropy, 'r', label ='Cross Entropy')\n",
        "plt.plot(epochs, val_accuracy_squareloss, 'b', label ='Squared Loss')\n",
        "plt.legend()\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Validation accuracy per epoch \")\n",
        "plt.title('Validation accuracy comparison for Squared Loss and Crossentropy Loss', fontsize=20)\n",
        "wandb.log({\"Validation accuracy per epoch for Squared v/s CrossEntropy Loss \": wandb.Image(plt)})\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCY6_z22a1QQ"
      },
      "source": [
        "Question 10 (10 Marks)\n",
        "\n",
        "Based on your learnings above, give me 3 recommendations for what would work for the MNIST dataset (not Fashion-MNIST). Just to be clear, I am asking you to take your learnings based on extensive experimentation with one dataset and see if these learnings help on another dataset. If I give you a budget of running only 3 hyperparameter configurations as opposed to the large number of experiments you have run above then which 3 would you use and why. Report the accuracies that you obtain using these 3 configurations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Th0fhoeka0n2"
      },
      "outputs": [],
      "source": [
        "#loading the mnist dataset\n",
        "(X_train_mnist_full, Y_train_mnist_full), (X_test_mnist, Y_test_mnist) = mnist.load_data()\n",
        "\n",
        "#lets do normalization of the data\n",
        "X_train_mnist_full = X_train_mnist_full/255.0\n",
        "X_test_mnist = X_test_mnist/255.0\n",
        "\n",
        "#Let's split the data\n",
        "X_train_mnist, X_val_mnist, Y_train_mnist, Y_val_mnist = train_test_split(X_train_mnist_full, Y_train_mnist_full, test_size=0.1, random_state=137)\n",
        "\n",
        "Y_train_mnist_unencoded = Y_train_mnist\n",
        "#converting the labels in one hot form\n",
        "encoder = OneHotEncoder()\n",
        "Y_train_mnist = encoder.fit_transform(np.expand_dims(Y_train_mnist,1)).toarray()\n",
        "Y_val_mnist = encoder.fit_transform(np.expand_dims(Y_val_mnist,1)).toarray()\n",
        "Y_test_mnist = encoder.fit_transform(np.expand_dims(Y_test_mnist,1)).toarray()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ITP2Uq0a6Qr"
      },
      "outputs": [],
      "source": [
        "#Add layer sizes for the hidden layers\n",
        "layers_3 = [32, 32, 32]\n",
        "layers_4 = [32, 32, 32, 32]\n",
        "\n",
        "optimizers = {\"gradient_descent\":StocasticGD(), \"momentum_gd\":MomentumGD(), \"nag\":NAG(), \"rmsprop\":RMSProp(), \"adam\":Adam(), \"nadam\":Nadam()}\n",
        "loss_functions = {\"cross_entropy\":CrossEntropy(), \"squared_loss\":SquaredErrorLoss()}\n",
        "activation_functions = {\"sigmoid\": Sigmoid(), \"softmax\":Softmax(), \"tanh\": Tanh(), \"ReLU\":ReLU()}\n",
        "\n",
        "#Select optimizer(sgd)\n",
        "optimizer_sgd = optimizers[\"gradient_descent\"]\n",
        "optimizer_parameters_sgd = {\"learning_rate\":0.001, \"weight_decay\":0.0005}\n",
        "optimizer_sgd.set_initial_parameters(optimizer_parameters_sgd)\n",
        "\n",
        "#Select optimizer(adam)\n",
        "optimizer_adam = optimizers[\"adam\"]\n",
        "optimizer_parameters_adam = {\"learning_rate\":0.001, \"beta1\":0.09, \"beta2\":0.999, \"epsilon\":1e-8, \"weight_decay\":0.0005}\n",
        "optimizer_adam.set_initial_parameters(optimizer_parameters_adam)\n",
        "\n",
        "#Select optimizer(nadam)\n",
        "optimizer_nadam = optimizers[\"nadam\"]\n",
        "optimizer_parameters_nadam = {\"learning_rate\":0.001, \"beta1\":0.09, \"beta2\":0.999, \"epsilon\":1e-8, \"weight_decay\":0.0005}\n",
        "optimizer_nadam.set_initial_parameters(optimizer_parameters_nadam)\n",
        "\n",
        "\n",
        "#Select loss function\n",
        "loss_cross_entropy = loss_functions[\"cross_entropy\"]\n",
        "loss_squared = loss_functions[\"squared_loss\"]\n",
        "\n",
        "#Select activation(hidden layers)\n",
        "activation_tanh = activation_functions[\"tanh\"]\n",
        "activation_ReLU = activation_functions[\"ReLU\"]\n",
        "\n",
        "\n",
        "#Select activation(output layer)\n",
        "output_activation_softmax = activation_functions[\"softmax\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xe9aSfdDbExo"
      },
      "outputs": [],
      "source": [
        "model_mnist_1 = FeedForwardNN(layers_3, optimizer_adam, ReLU(), Softmax(), CrossEntropy(), 15, 64, initialization = \"Xavier\")\n",
        "model_mnist_1.fit(X_train_mnist, Y_train_mnist, X_val_mnist, Y_val_mnist)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j8Lwhe3sbQK3"
      },
      "outputs": [],
      "source": [
        "model_mnist_2 = FeedForwardNN(layers_3, optimizer_sgd, Tanh(), Softmax(), CrossEntropy(), 15, 32, initialization = \"Xavier\")\n",
        "model_mnist_2.fit(X_train_mnist, Y_train_mnist, X_val_mnist, Y_val_mnist)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0-LvLc0vbYJ2"
      },
      "outputs": [],
      "source": [
        "model_mnist_3 = FeedForwardNN(layers_3, optimizer_nadam, Tanh(), Softmax(), CrossEntropy(), 16, 64, initialization = \"Xavier\")\n",
        "model_mnist_3.fit(X_train_mnist, Y_train_mnist, X_val_mnist, Y_val_mnist)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1cN6x6x2bd_w"
      },
      "outputs": [],
      "source": [
        "y_pred_mnist_1 = model_mnist_1.predict(X_test_mnist)\n",
        "print(\"Test Accuracy for model 1:\", accuracy(np.argmax(Y_test_mnist,1), np.argmax(y_pred_mnist_1,1)))\n",
        "\n",
        "y_pred_mnist_2 = model_mnist_2.predict(X_test_mnist)\n",
        "print(\"Test Accuracy for model 2:\", accuracy(np.argmax(Y_test_mnist,1), np.argmax(y_pred_mnist_2,1)))\n",
        "\n",
        "y_pred_mnist_3 = model_mnist_3.predict(X_test_mnist)\n",
        "print(\"Test Accuracy for model 3:\", accuracy(np.argmax(Y_test_mnist,1), np.argmax(y_pred_mnist_3,1)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mM4nOssdGOgz"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}