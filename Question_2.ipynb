{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 2 (10 Marks)\n",
        "\n",
        "Implement a feedforward neural network which takes images from the fashion-mnist data as input and outputs a probability distribution over the 10 classes.\n",
        "\n",
        "Your code should be flexible such that it is easy to change the number of hidden layers and the number of neurons in each hidden layer."
      ],
      "metadata": {
        "id": "F4yVr0dxk3af"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "LJOL4KsnQFeO"
      },
      "outputs": [],
      "source": [
        "#Importing all the libraries that will be used\n",
        "from keras.datasets import fashion_mnist\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "import random\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Load the fashion MNIST data\n",
        "\n",
        "\n",
        "(X_train, Y_train), (X_test, Y_test) = fashion_mnist.load_data()\n",
        "#Normalizing the data\n",
        "X_train_com = X_train/255.0\n",
        "X_test = X_test/255.0\n",
        "np.random.seed(137)\n",
        "encoder = OneHotEncoder()\n",
        "\n",
        "#Splitting to get 10% data as validation set\n",
        "#X_train, X_val, Y_train, Y_val = train_test_split(X_train_com, Y_train_com, test_size=0.1, random_state=137)\n",
        "\n",
        "Y_train_unencoded = Y_train\n",
        "#One hot encoding of the class labels\n",
        "\n",
        "\n",
        "\n",
        "Y_train = encoder.fit_transform(np.expand_dims(Y_train,1)).toarray()\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wLquSd2PQGs4"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForwardNN():\n",
        "  \"\"\"\n",
        "  This class implements the forward propagation functionality of a feedforward neural network. It supports flexible hidden layer sizes and four different activation functions.\n",
        "\n",
        "  Attributes:\n",
        "      hidden_layer_sizes: A list containing the sizes of the hidden layers (the length of the list indicates the number of hidden layers in the network).\n",
        "  \"\"\"\n",
        "  def __init__(self, hidden_layer_sizes):\n",
        "    self.weights = {}\n",
        "    self.biases = {}\n",
        "    self.hidden_layer_sizes = hidden_layer_sizes\n",
        "    self.input_layer_size=0\n",
        "    self.output_layer_size=1\n",
        "    # The array of layer sizes will be initialized after obtaining the input and output layer sizes\n",
        "    self.layer_sizes = []\n",
        "\n",
        "    self.A = {}\n",
        "    self.H = {}\n",
        "\n",
        "\n",
        "\n",
        "  def initialize_weights(self):\n",
        "    \"\"\"\n",
        "    Initializes the weights between the layers of the network. Weights are randomly initialized.\n",
        "    \"\"\"\n",
        "    self.layer_sizes = [self.input_layer_size] + self.hidden_layer_sizes + [self.output_layer_size]\n",
        "    len_hl=len(self.hidden_layer_sizes)\n",
        "    weight_counts = len_hl +1\n",
        "    # np.random.seed(137)\n",
        "    # np.random.RandomState(137)\n",
        "    i=0\n",
        "    while i<(weight_counts):\n",
        "      # Initialize weights for each layer randomly as a matrix of size (previous layer size) * (next layer size)\n",
        "      self.weights[i+1] = np.random.randn(self.layer_sizes[i], self.layer_sizes[i+1])\n",
        "     # Initialize biases for each layer to 0 as a matrix of size 1 * (next layer size)\n",
        "      self.biases[i+1] = np.zeros((1, self.layer_sizes[i+1]))\n",
        "      i+=1\n",
        "\n",
        "  def calculate_activation(self, name, X):\n",
        "\n",
        "    if name==\"ReLU\":\n",
        "      s=self.ReLU(X)\n",
        "      return s\n",
        "\n",
        "    if name==\"sigmoid\":\n",
        "      s=self.sigmoid(X)\n",
        "      return s\n",
        "\n",
        "    if name==\"identity\":\n",
        "      s=self.identity(X)\n",
        "      return s\n",
        "\n",
        "\n",
        "    if name==\"tanh\":\n",
        "      s=self.tanh(X)\n",
        "      return s\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  #calculate the ReLU function\n",
        "  def ReLU(self,X):\n",
        "    return X * (X > 0)\n",
        "  # calculating the softmax function\n",
        "\n",
        "  #function used\n",
        "  def u1():\n",
        "    return 1\n",
        "  def softmax(self, X):\n",
        "    X_max = np.max(X)\n",
        "    exponentials = np.exp(X - X_max)\n",
        "    return exponentials / np.sum(exponentials)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  # Utility function to calculate the sigmoid function\n",
        "  def sigmoid(self, X):\n",
        "    clipped_X = np.clip(X, -500, 500)\n",
        "    return 1.0 / (1.0 + np.exp(-clipped_X))\n",
        "\n",
        "  def identity(self, X):\n",
        "  # Utility to calculate identity function\n",
        "    return X\n",
        "\n",
        "  #calculating the cross entropy funtion\n",
        "  def cross_entropy(self,Y_true,Y_pred):\n",
        "    loss=np.multiply(Y_pred,Y_true)\n",
        "    loss=loss[loss!=0]\n",
        "    loss=-np.log(loss)\n",
        "    loss=np.mean(loss)\n",
        "    return loss\n",
        "\n",
        "\n",
        "\n",
        "  #calculating the tanh funtion\n",
        "  def tanh(self,X):\n",
        "    return np.tanh(X)\n",
        "\n",
        "\n",
        "\n",
        "  \"\"\"\n",
        "   Performing forward propagation in the data X.\n",
        "  \"\"\"\n",
        "  def forward_prop(self, activation, X):\n",
        "\n",
        "\n",
        "    self.H = {}\n",
        "    #set i=0 for the while loop\n",
        "    i=0\n",
        "    self.A = {}\n",
        "    #Make a row vector intialize the output from input layer as H[0] into a single row\n",
        "    self.H[0] = X.reshape(1,-1)\n",
        "\n",
        "    #formula for calculation\n",
        "    #formula applied:a(x) = W_x*h(x-1) + b and h(a(x)) = activation(a(x)) for hidden layer\n",
        "\n",
        "    while i<(len(self.hidden_layer_sizes)):\n",
        "      mat = np.matmul(self.H[i], self.weights[i+1])\n",
        "      self.A[i+1] =  mat + self.biases[i+1]\n",
        "      cal=self.calculate_activation(activation, self.A[i+1])\n",
        "      self.H[i+1] = cal\n",
        "      i+=1\n",
        "\n",
        "    # calculating hadamard and softmax at the output layer\n",
        "    # for the output layer a(x) and h(a(x)) = softmax(a(x))\n",
        "    mul=np.matmul(self.H[len(self.hidden_layer_sizes)], self.weights[len(self.hidden_layer_sizes)+1])\n",
        "    self.A[len(self.hidden_layer_sizes)+1] = mul + self.biases[len(self.hidden_layer_sizes)+1]\n",
        "\n",
        "    self.H[len(self.hidden_layer_sizes)+1] = self.softmax(self.A[len(self.hidden_layer_sizes)+1])\n",
        "    return\n",
        "\n",
        "\n",
        "  def fit(self, activation, X, Y):\n",
        "    \"\"\"\n",
        "    This method trains the model with the given data (X, Y) by performing a single forward pass and producing the probabilities calculated at the output layer.\n",
        "    \"\"\"\n",
        "\n",
        "    # columns in output (label count)\n",
        "    self.output_layer_size = Y.shape[1]\n",
        "\n",
        "    # features in data(features)\n",
        "    self.input_layer_size = X.shape[1]*X.shape[1]\n",
        "\n",
        "    output_prob = []\n",
        "    self.initialize_weights()\n",
        "\n",
        "    for x, y in zip(X, Y):\n",
        "      #Forward Propogation\n",
        "      self.forward_prop(activation, x)\n",
        "\n",
        "      output_prob.append(self.H[len(self.hidden_layer_sizes)+1][0])\n",
        "    return output_prob\n",
        "\n"
      ],
      "metadata": {
        "id": "ozRjTWwVQk33"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Sample implementation using 3 hidden layers of sizes 100, 256 and 512 respectively\n",
        "\n",
        "model = FeedForwardNN([128,128,128])\n",
        "activation_name = \"sigmoid\"\n",
        "class_predictions = model.fit(activation_name, X_train, Y_train)\n",
        "#Class probabilities for 0th indexed image\n",
        "\n",
        "np.set_printoptions(suppress=True)\n",
        "print(class_predictions[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JWvopE9uQpaR",
        "outputId": "a67057a2-c2fa-4971-82b2-9562814fffdf"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.00126001 0.42737141 0.51574302 0.00017754 0.00000039 0.00000002\n",
            " 0.00138624 0.00199597 0.00003026 0.05203515]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "scUD3yDDQsmJ"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z8wEoaNghPyT"
      },
      "execution_count": 101,
      "outputs": []
    }
  ]
}